import logging
import pandas as pd
import torch
from flask import Flask, Response
import requests
import json
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from chronos import ChronosPipeline
from datetime import datetime, timedelta
import time

logging.basicConfig(level=logging.INFO)

# create our Flask app
app = Flask(__name__)

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# define the Hugging Face model we will use
model_name = "t5-small"  # Utilisons un modèle T5 standard à la place

# Load model and tokenizer
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map="auto")
except Exception as e:
    app.logger.error(f"Erreur de chargement du modèle ou du tokenizer: {e}")
    raise

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Cache pour stocker les données Binance
binance_cache = {}

def get_cached_binance_data(symbol, interval='1d', limit=30):
    cache_key = f"{symbol}_{interval}_{limit}"
    current_time = time.time()
    
    if cache_key in binance_cache:
        cached_data, timestamp = binance_cache[cache_key]
        if current_time - timestamp < 300:  # 5 minutes cache
            return cached_data
    
    data = get_binance_data(symbol, interval, limit)
    if data is not None:
        binance_cache[cache_key] = (data, current_time)
    return data

def get_binance_data(symbol, interval='1d', limit=30):
    base_url = "https://api.binance.com/api/v3/klines"
    params = {
        "symbol": f"{symbol}USDT",
        "interval": interval,
        "limit": limit
    }
    max_retries = 3
    retry_delay = 5

    for attempt in range(max_retries):
        try:
            response = requests.get(base_url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df['close'] = df['close'].astype(float)
            return df[['timestamp', 'close']]
        except requests.RequestException as e:
            app.logger.error(f"Attempt {attempt + 1} failed: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
            else:
                app.logger.error("Max retries reached. Unable to get data from Binance API.")
                return None

@app.route("/inference/<string:token>")
def get_inference(token):
    """Generate inference for given token."""
    df = get_cached_binance_data(token)
    if df is None:
        return Response(json.dumps({"error": "Failed to retrieve data from Binance API"}), 
                        status=500, 
                        mimetype='application/json')

    app.logger.info(f"Data retrieved for {token}: {df.tail(5)}")

    # Prepare input for the model
    input_data = df['close'].tolist()[-5:]  # Utilisons les 5 derniers prix
    input_text = f"Predict the next price for {token} given the following prices: {' '.join(map(str, input_data))}"

    try:
        # Generate prediction
        inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True).to(device)
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=50)
        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract the last number from the prediction
        result = float(prediction.split()[-1])
        
        app.logger.info(f"Forecast for {token}: {result}")
        return Response(str(result), status=200)

    except Exception as e:
        app.logger.error(f"Prediction error: {str(e)}")
        return Response(json.dumps({"error": str(e)}), status=500, mimetype='application/json')
# run our Flask app
if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8000, debug=True)